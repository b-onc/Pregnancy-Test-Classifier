{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'dataset/train'\n",
    "validation_data_dir = 'dataset/test'\n",
    "nb_train_samples = 2048\n",
    "nb_validation_samples = 512\n",
    "\n",
    "generator_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_with_input_shape(img_width, img_height):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generators(img_width, img_height, batch_size, rotation_range, zoom_range):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=zoom_range,\n",
    "        rotation_range=rotation_range,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        zoom_range=zoom_range,\n",
    "        rotation_range=rotation_range)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        seed=generator_seed)\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        seed=generator_seed)\n",
    "    \n",
    "    return (train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, validation_generator, batch_size, epochs):\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, validation_generator, batch_size):\n",
    "    score = model.evaluate_generator(validation_generator, steps=nb_validation_samples // batch_size, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = [256, 512]\n",
    "batch_sizes = [16, 32]\n",
    "epochs = [4, 8]\n",
    "rotation_ranges = [40, 50]\n",
    "zoom_ranges = [0.20, 0.30, 0.40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying model with image_size:256, batch_size:16, epochs:4, rotation_range:40, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 164s 1s/step - loss: 1.6136 - acc: 0.7109 - val_loss: 0.6317 - val_acc: 0.6589\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.4499 - acc: 0.7681 - val_loss: 1.1285 - val_acc: 0.7448\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.2652 - acc: 0.8927 - val_loss: 2.0740 - val_acc: 0.7734\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.2018 - acc: 0.9238 - val_loss: 3.6694 - val_acc: 0.7057\n",
      "---\n",
      "32/32 [==============================] - 25s 790ms/step\n",
      "Test loss: 3.519182190299034\n",
      "Test accuracy: 0.7135416753590107\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:4, rotation_range:40, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 1019s 8s/step - loss: 0.6366 - acc: 0.7286 - val_loss: 0.9314 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 152s 1s/step - loss: 0.3877 - acc: 0.8326 - val_loss: 1.2247 - val_acc: 0.6042\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2519 - acc: 0.9039 - val_loss: 2.3094 - val_acc: 0.6979\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1917 - acc: 0.9413 - val_loss: 2.4513 - val_acc: 0.6354\n",
      "---\n",
      "32/32 [==============================] - 26s 801ms/step\n",
      "Test loss: 2.273631837219\n",
      "Test accuracy: 0.6432291716337204\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:4, rotation_range:40, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.6410 - acc: 0.7166 - val_loss: 0.7051 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.4275 - acc: 0.7863 - val_loss: 0.7028 - val_acc: 0.6953\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.3258 - acc: 0.8585 - val_loss: 0.6524 - val_acc: 0.6120\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2315 - acc: 0.9063 - val_loss: 2.6450 - val_acc: 0.7318\n",
      "---\n",
      "32/32 [==============================] - 26s 798ms/step\n",
      "Test loss: 2.4221830293536186\n",
      "Test accuracy: 0.7343750037252903\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:4, rotation_range:50, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 154s 1s/step - loss: 0.6844 - acc: 0.7236 - val_loss: 0.6965 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.3484 - acc: 0.8522 - val_loss: 0.7682 - val_acc: 0.6094\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2601 - acc: 0.9199 - val_loss: 3.2539 - val_acc: 0.5833\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1881 - acc: 0.9445 - val_loss: 4.3607 - val_acc: 0.5859\n",
      "---\n",
      "32/32 [==============================] - 25s 796ms/step\n",
      "Test loss: 4.352577447891235\n",
      "Test accuracy: 0.5807291604578495\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:4, rotation_range:50, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.6499 - acc: 0.7195 - val_loss: 0.5704 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.4336 - acc: 0.7907 - val_loss: 0.5370 - val_acc: 0.6380\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.2975 - acc: 0.8710 - val_loss: 2.6286 - val_acc: 0.6276\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2270 - acc: 0.9142 - val_loss: 1.6597 - val_acc: 0.6120\n",
      "---\n",
      "32/32 [==============================] - 26s 799ms/step\n",
      "Test loss: 1.5089157111942768\n",
      "Test accuracy: 0.6093749981373549\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:4, rotation_range:50, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.6702 - acc: 0.7098 - val_loss: 0.6007 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.4741 - acc: 0.7861 - val_loss: 0.8347 - val_acc: 0.6771\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.3315 - acc: 0.8599 - val_loss: 1.2334 - val_acc: 0.5833\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2767 - acc: 0.8937 - val_loss: 2.0523 - val_acc: 0.6276\n",
      "---\n",
      "32/32 [==============================] - 26s 798ms/step\n",
      "Test loss: 1.9117746613919735\n",
      "Test accuracy: 0.6380208395421505\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:8, rotation_range:40, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 154s 1s/step - loss: 0.6086 - acc: 0.7352 - val_loss: 0.8766 - val_acc: 0.6901\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.3202 - acc: 0.8648 - val_loss: 1.3500 - val_acc: 0.5104\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1960 - acc: 0.9328 - val_loss: 1.9921 - val_acc: 0.5026\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1157 - acc: 0.9617 - val_loss: 4.0427 - val_acc: 0.6276\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1270 - acc: 0.9685 - val_loss: 2.5103 - val_acc: 0.7005\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.0824 - acc: 0.9762 - val_loss: 3.3697 - val_acc: 0.6250\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.0905 - acc: 0.9814 - val_loss: 3.5657 - val_acc: 0.6667\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.0566 - acc: 0.9840 - val_loss: 4.0314 - val_acc: 0.6276\n",
      "---\n",
      "32/32 [==============================] - 26s 810ms/step\n",
      "Test loss: 4.0366965010762215\n",
      "Test accuracy: 0.6328125037252903\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:8, rotation_range:40, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.6414 - acc: 0.7168 - val_loss: 0.6672 - val_acc: 0.5833\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.4342 - acc: 0.7694 - val_loss: 0.7877 - val_acc: 0.6484\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2758 - acc: 0.8809 - val_loss: 1.1980 - val_acc: 0.6693\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2078 - acc: 0.9304 - val_loss: 2.7451 - val_acc: 0.6979\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1738 - acc: 0.9490 - val_loss: 2.7229 - val_acc: 0.6458\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1176 - acc: 0.9704 - val_loss: 3.0482 - val_acc: 0.7083\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1094 - acc: 0.9683 - val_loss: 3.3202 - val_acc: 0.6432\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1067 - acc: 0.9767 - val_loss: 3.3048 - val_acc: 0.7240\n",
      "---\n",
      "32/32 [==============================] - 26s 798ms/step\n",
      "Test loss: 3.127043977379799\n",
      "Test accuracy: 0.7109375055879354\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:8, rotation_range:40, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 154s 1s/step - loss: 0.5972 - acc: 0.7209 - val_loss: 0.6299 - val_acc: 0.6406\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.3946 - acc: 0.8150 - val_loss: 1.1686 - val_acc: 0.7109\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 143s 1s/step - loss: 0.2594 - acc: 0.8981 - val_loss: 1.4357 - val_acc: 0.6380\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1801 - acc: 0.9362 - val_loss: 1.9474 - val_acc: 0.6250\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.1265 - acc: 0.9632 - val_loss: 2.0910 - val_acc: 0.6979\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1211 - acc: 0.9577 - val_loss: 2.8689 - val_acc: 0.6250\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1293 - acc: 0.9641 - val_loss: 3.0476 - val_acc: 0.6380\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.1653 - acc: 0.9706 - val_loss: 2.7943 - val_acc: 0.5729\n",
      "---\n",
      "32/32 [==============================] - 25s 794ms/step\n",
      "Test loss: 2.585416592657566\n",
      "Test accuracy: 0.5624999916180968\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:8, rotation_range:50, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 154s 1s/step - loss: 0.6526 - acc: 0.7266 - val_loss: 0.6175 - val_acc: 0.6224\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.4140 - acc: 0.7958 - val_loss: 0.8516 - val_acc: 0.7422\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2826 - acc: 0.8820 - val_loss: 1.4705 - val_acc: 0.6615\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1865 - acc: 0.9265 - val_loss: 2.3287 - val_acc: 0.6250\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.1372 - acc: 0.9576 - val_loss: 2.2690 - val_acc: 0.6172\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.0965 - acc: 0.9647 - val_loss: 2.5816 - val_acc: 0.5547\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1326 - acc: 0.9710 - val_loss: 2.8715 - val_acc: 0.5625\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.0758 - acc: 0.9806 - val_loss: 2.4659 - val_acc: 0.5573\n",
      "---\n",
      "32/32 [==============================] - 25s 795ms/step\n",
      "Test loss: 2.2655713446438313\n",
      "Test accuracy: 0.5807291623204947\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:8, rotation_range:50, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 154s 1s/step - loss: 0.6190 - acc: 0.7177 - val_loss: 0.6647 - val_acc: 0.6276\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.4014 - acc: 0.8036 - val_loss: 0.9629 - val_acc: 0.7031\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2547 - acc: 0.8950 - val_loss: 1.5234 - val_acc: 0.6927\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1533 - acc: 0.9442 - val_loss: 2.0875 - val_acc: 0.7005\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1176 - acc: 0.9621 - val_loss: 1.9753 - val_acc: 0.7214\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.1387 - acc: 0.9678 - val_loss: 2.8599 - val_acc: 0.6641\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.0944 - acc: 0.9699 - val_loss: 3.4095 - val_acc: 0.6276\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1043 - acc: 0.9671 - val_loss: 3.7386 - val_acc: 0.5703\n",
      "---\n",
      "32/32 [==============================] - 25s 791ms/step\n",
      "Test loss: 3.6111761927604675\n",
      "Test accuracy: 0.5651041502133012\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:16, epochs:8, rotation_range:50, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 153s 1s/step - loss: 0.6734 - acc: 0.7159 - val_loss: 0.8948 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.4998 - acc: 0.7591 - val_loss: 0.8681 - val_acc: 0.6302\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.3485 - acc: 0.8324 - val_loss: 1.0608 - val_acc: 0.7344\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2918 - acc: 0.8870 - val_loss: 1.1775 - val_acc: 0.7266\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.2246 - acc: 0.9247 - val_loss: 1.0858 - val_acc: 0.7656\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1515 - acc: 0.9431 - val_loss: 3.0747 - val_acc: 0.5937\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1485 - acc: 0.9545 - val_loss: 2.3719 - val_acc: 0.5651\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1194 - acc: 0.9590 - val_loss: 3.3256 - val_acc: 0.5104\n",
      "---\n",
      "32/32 [==============================] - 25s 785ms/step\n",
      "Test loss: 3.3923751786351204\n",
      "Test accuracy: 0.5052083283662796\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:4, rotation_range:40, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 105s 2s/step - loss: 0.9821 - acc: 0.6673 - val_loss: 0.5861 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.5547 - acc: 0.7303 - val_loss: 0.5545 - val_acc: 0.5990\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 101s 2s/step - loss: 0.4533 - acc: 0.7611 - val_loss: 1.2097 - val_acc: 0.4687\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 101s 2s/step - loss: 0.4493 - acc: 0.7703 - val_loss: 1.1016 - val_acc: 0.5365\n",
      "---\n",
      "16/16 [==============================] - 13s 801ms/step\n",
      "Test loss: 1.1921306066215038\n",
      "Test accuracy: 0.5208333283662796\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:4, rotation_range:40, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 105s 2s/step - loss: 1.3767 - acc: 0.7022 - val_loss: 0.5603 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 101s 2s/step - loss: 0.5918 - acc: 0.7223 - val_loss: 0.5415 - val_acc: 0.6979\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4767 - acc: 0.7628 - val_loss: 0.7111 - val_acc: 0.6094\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4057 - acc: 0.7961 - val_loss: 1.3965 - val_acc: 0.5677\n",
      "---\n",
      "16/16 [==============================] - 13s 807ms/step\n",
      "Test loss: 1.5096216797828674\n",
      "Test accuracy: 0.5937499888241291\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:4, rotation_range:40, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 106s 2s/step - loss: 1.3678 - acc: 0.6941 - val_loss: 0.5843 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.6105 - acc: 0.7142 - val_loss: 0.5504 - val_acc: 0.6979\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4849 - acc: 0.7619 - val_loss: 0.5285 - val_acc: 0.6875\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4548 - acc: 0.7845 - val_loss: 0.5181 - val_acc: 0.6406\n",
      "---\n",
      "16/16 [==============================] - 13s 799ms/step\n",
      "Test loss: 0.530611764639616\n",
      "Test accuracy: 0.6406250074505806\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:4, rotation_range:50, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 107s 2s/step - loss: 1.3866 - acc: 0.6998 - val_loss: 0.5861 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.5247 - acc: 0.7399 - val_loss: 0.4994 - val_acc: 0.7656\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4246 - acc: 0.7840 - val_loss: 0.5779 - val_acc: 0.5833\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.3453 - acc: 0.8326 - val_loss: 0.5411 - val_acc: 0.5990\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 13s 801ms/step\n",
      "Test loss: 0.5364042222499847\n",
      "Test accuracy: 0.6145833283662796\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:4, rotation_range:50, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 106s 2s/step - loss: 1.3445 - acc: 0.6990 - val_loss: 0.5724 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.5966 - acc: 0.7155 - val_loss: 0.4869 - val_acc: 0.7344\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 101s 2s/step - loss: 0.5061 - acc: 0.7467 - val_loss: 0.5338 - val_acc: 0.6354\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4829 - acc: 0.7629 - val_loss: 0.5101 - val_acc: 0.6042\n",
      "---\n",
      "16/16 [==============================] - 13s 802ms/step\n",
      "Test loss: 0.5065992418676615\n",
      "Test accuracy: 0.5781249925494194\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:4, rotation_range:50, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 107s 2s/step - loss: 1.3512 - acc: 0.7006 - val_loss: 0.5914 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.5974 - acc: 0.7205 - val_loss: 0.5250 - val_acc: 0.7500\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4899 - acc: 0.7556 - val_loss: 0.5573 - val_acc: 0.5521\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4390 - acc: 0.7616 - val_loss: 0.5083 - val_acc: 0.5937\n",
      "---\n",
      "16/16 [==============================] - 13s 801ms/step\n",
      "Test loss: 0.5152212101966143\n",
      "Test accuracy: 0.5572916641831398\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:8, rotation_range:40, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 106s 2s/step - loss: 1.3554 - acc: 0.6959 - val_loss: 0.5485 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.5915 - acc: 0.7266 - val_loss: 0.5022 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4142 - acc: 0.7875 - val_loss: 0.6036 - val_acc: 0.6146\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 101s 2s/step - loss: 0.3880 - acc: 0.8134 - val_loss: 1.2980 - val_acc: 0.4062\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.2799 - acc: 0.8860 - val_loss: 0.9851 - val_acc: 0.6302\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.2124 - acc: 0.9215 - val_loss: 0.8416 - val_acc: 0.5208\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.2211 - acc: 0.9441 - val_loss: 1.6169 - val_acc: 0.6198\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 101s 2s/step - loss: 0.1303 - acc: 0.9584 - val_loss: 2.2643 - val_acc: 0.5104\n",
      "---\n",
      "16/16 [==============================] - 13s 799ms/step\n",
      "Test loss: 2.2142595797777176\n",
      "Test accuracy: 0.5208333283662796\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:8, rotation_range:40, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 106s 2s/step - loss: 0.7642 - acc: 0.7022 - val_loss: 0.5862 - val_acc: 0.6771\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.5161 - acc: 0.7539 - val_loss: 0.8321 - val_acc: 0.6146\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.3849 - acc: 0.8286 - val_loss: 1.3887 - val_acc: 0.4635\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.2945 - acc: 0.8761 - val_loss: 1.1495 - val_acc: 0.6719\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.2143 - acc: 0.9129 - val_loss: 4.4451 - val_acc: 0.5885\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.1765 - acc: 0.9375 - val_loss: 3.0261 - val_acc: 0.6875\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 100s 2s/step - loss: 0.1621 - acc: 0.9416 - val_loss: 2.2701 - val_acc: 0.7448\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.1350 - acc: 0.9638 - val_loss: 1.1388 - val_acc: 0.6406\n",
      "---\n",
      "16/16 [==============================] - 13s 804ms/step\n",
      "Test loss: 1.1371791344136\n",
      "Test accuracy: 0.6041666604578495\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:8, rotation_range:40, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 107s 2s/step - loss: 0.7509 - acc: 0.6937 - val_loss: 0.5837 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.5272 - acc: 0.7549 - val_loss: 0.5986 - val_acc: 0.6615\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 101s 2s/step - loss: 0.3900 - acc: 0.8219 - val_loss: 0.9190 - val_acc: 0.6198\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.3098 - acc: 0.8693 - val_loss: 1.0428 - val_acc: 0.4896\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.2611 - acc: 0.9077 - val_loss: 1.7302 - val_acc: 0.7240\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.1882 - acc: 0.9347 - val_loss: 2.7542 - val_acc: 0.6563\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.1838 - acc: 0.9402 - val_loss: 2.1243 - val_acc: 0.5781\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.1512 - acc: 0.9576 - val_loss: 4.1735 - val_acc: 0.5937\n",
      "---\n",
      "16/16 [==============================] - 13s 808ms/step\n",
      "Test loss: 4.092280149459839\n",
      "Test accuracy: 0.6197916679084301\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:8, rotation_range:50, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 107s 2s/step - loss: 0.7316 - acc: 0.6947 - val_loss: 0.5439 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.5721 - acc: 0.7270 - val_loss: 0.5808 - val_acc: 0.6146\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 103s 2s/step - loss: 0.4641 - acc: 0.7770 - val_loss: 0.5293 - val_acc: 0.6042\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.3173 - acc: 0.8383 - val_loss: 0.8411 - val_acc: 0.5885\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 104s 2s/step - loss: 0.2912 - acc: 0.8737 - val_loss: 1.4676 - val_acc: 0.6719\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 103s 2s/step - loss: 0.1838 - acc: 0.9397 - val_loss: 3.8870 - val_acc: 0.5312\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 103s 2s/step - loss: 0.1898 - acc: 0.9370 - val_loss: 1.2772 - val_acc: 0.6146\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 103s 2s/step - loss: 0.1395 - acc: 0.9562 - val_loss: 1.9576 - val_acc: 0.5521\n",
      "---\n",
      "16/16 [==============================] - 13s 804ms/step\n",
      "Test loss: 1.7753496766090393\n",
      "Test accuracy: 0.546874986961484\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:8, rotation_range:50, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 107s 2s/step - loss: 0.7273 - acc: 0.6934 - val_loss: 0.6071 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 103s 2s/step - loss: 0.5365 - acc: 0.7506 - val_loss: 0.5517 - val_acc: 0.6094\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 103s 2s/step - loss: 0.4087 - acc: 0.8135 - val_loss: 1.0891 - val_acc: 0.4948\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 103s 2s/step - loss: 0.2860 - acc: 0.8712 - val_loss: 1.0757 - val_acc: 0.6146\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 103s 2s/step - loss: 0.2262 - acc: 0.9150 - val_loss: 1.0537 - val_acc: 0.6042\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.1359 - acc: 0.9542 - val_loss: 2.4566 - val_acc: 0.6927\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.1647 - acc: 0.9491 - val_loss: 2.2574 - val_acc: 0.5937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "64/64 [==============================] - 104s 2s/step - loss: 0.1492 - acc: 0.9508 - val_loss: 2.5948 - val_acc: 0.5417\n",
      "---\n",
      "16/16 [==============================] - 13s 810ms/step\n",
      "Test loss: 2.2509490847587585\n",
      "Test accuracy: 0.5572916567325592\n",
      "-----\n",
      "Trying model with image_size:256, batch_size:32, epochs:8, rotation_range:50, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 107s 2s/step - loss: 1.6392 - acc: 0.6793 - val_loss: 0.6034 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.6259 - acc: 0.7164 - val_loss: 0.5131 - val_acc: 0.7240\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 103s 2s/step - loss: 0.4895 - acc: 0.7539 - val_loss: 1.0549 - val_acc: 0.5260\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.4443 - acc: 0.8070 - val_loss: 1.2175 - val_acc: 0.5469\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 101s 2s/step - loss: 0.3512 - acc: 0.8427 - val_loss: 0.7382 - val_acc: 0.6979\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.2314 - acc: 0.8954 - val_loss: 2.1573 - val_acc: 0.6875\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.2537 - acc: 0.8944 - val_loss: 1.5688 - val_acc: 0.6875\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 102s 2s/step - loss: 0.1821 - acc: 0.9320 - val_loss: 3.4498 - val_acc: 0.7031\n",
      "---\n",
      "16/16 [==============================] - 13s 803ms/step\n",
      "Test loss: 3.483396455645561\n",
      "Test accuracy: 0.6666666716337204\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:4, rotation_range:40, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 260s 2s/step - loss: 4.4813 - acc: 0.7153 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 4.4928 - acc: 0.7182 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 243s 2s/step - loss: 4.5349 - acc: 0.7155 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 4.5238 - acc: 0.7162 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 5.314128398895264\n",
      "Test accuracy: 0.6666666865348816\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:4, rotation_range:40, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 260s 2s/step - loss: 0.8290 - acc: 0.7118 - val_loss: 0.5610 - val_acc: 0.6354\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 241s 2s/step - loss: 0.4111 - acc: 0.8024 - val_loss: 1.0531 - val_acc: 0.6146\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 241s 2s/step - loss: 0.3203 - acc: 0.8742 - val_loss: 3.3695 - val_acc: 0.5964\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 0.2880 - acc: 0.8961 - val_loss: 2.3477 - val_acc: 0.6146\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 2.2552850022912025\n",
      "Test accuracy: 0.6119791669771075\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:4, rotation_range:40, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 260s 2s/step - loss: 0.7365 - acc: 0.7192 - val_loss: 0.6222 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 0.4389 - acc: 0.7893 - val_loss: 0.7383 - val_acc: 0.6354\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 0.3459 - acc: 0.8528 - val_loss: 2.0781 - val_acc: 0.4062\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 0.3144 - acc: 0.8765 - val_loss: 0.9426 - val_acc: 0.5885\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 0.8388570146635175\n",
      "Test accuracy: 0.6015624944120646\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:4, rotation_range:50, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 260s 2s/step - loss: 0.7151 - acc: 0.7198 - val_loss: 0.8537 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 241s 2s/step - loss: 0.3933 - acc: 0.8135 - val_loss: 1.1315 - val_acc: 0.6875\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 0.2640 - acc: 0.9003 - val_loss: 2.6922 - val_acc: 0.6146\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 243s 2s/step - loss: 0.3114 - acc: 0.9028 - val_loss: 1.3949 - val_acc: 0.6354\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 1.3098579309880733\n",
      "Test accuracy: 0.6458333395421505\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:4, rotation_range:50, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 260s 2s/step - loss: 0.7593 - acc: 0.7065 - val_loss: 0.7373 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 244s 2s/step - loss: 0.4245 - acc: 0.7981 - val_loss: 1.0406 - val_acc: 0.6146\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 0.3403 - acc: 0.8507 - val_loss: 2.4721 - val_acc: 0.5391\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 244s 2s/step - loss: 0.2754 - acc: 0.8982 - val_loss: 3.6969 - val_acc: 0.6641\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 3.6198074892163277\n",
      "Test accuracy: 0.638020833954215\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:4, rotation_range:50, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "128/128 [==============================] - 260s 2s/step - loss: 4.4816 - acc: 0.7139 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 1.1914 - acc: 0.7156 - val_loss: 0.8561 - val_acc: 0.6719\n",
      "Epoch 3/4\n",
      "128/128 [==============================] - 242s 2s/step - loss: 0.4554 - acc: 0.7829 - val_loss: 0.8010 - val_acc: 0.6458\n",
      "Epoch 4/4\n",
      "128/128 [==============================] - 243s 2s/step - loss: 0.3539 - acc: 0.8403 - val_loss: 1.3652 - val_acc: 0.5911\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 1.2866914737969637\n",
      "Test accuracy: 0.6041666641831398\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:8, rotation_range:40, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 261s 2s/step - loss: 0.7428 - acc: 0.7114 - val_loss: 0.5981 - val_acc: 0.6042\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 242s 2s/step - loss: 0.3957 - acc: 0.8253 - val_loss: 1.1578 - val_acc: 0.7734\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 239s 2s/step - loss: 0.2582 - acc: 0.9002 - val_loss: 2.6806 - val_acc: 0.6875\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 241s 2s/step - loss: 0.1974 - acc: 0.9217 - val_loss: 2.8928 - val_acc: 0.6406\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 244s 2s/step - loss: 0.2497 - acc: 0.9259 - val_loss: 2.8020 - val_acc: 0.6615\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 241s 2s/step - loss: 0.1705 - acc: 0.9428 - val_loss: 3.0053 - val_acc: 0.6380\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 243s 2s/step - loss: 0.2013 - acc: 0.9521 - val_loss: 3.3620 - val_acc: 0.6667\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 240s 2s/step - loss: 0.2203 - acc: 0.9614 - val_loss: 3.6476 - val_acc: 0.6328\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 3.6404958739876747\n",
      "Test accuracy: 0.6328125074505806\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:8, rotation_range:40, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 258s 2s/step - loss: 4.4809 - acc: 0.7168 - val_loss: 5.3141 - val_acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "128/128 [==============================] - 242s 2s/step - loss: 4.4928 - acc: 0.7182 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 242s 2s/step - loss: 4.5349 - acc: 0.7155 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 260s 2s/step - loss: 4.5238 - acc: 0.7162 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 243s 2s/step - loss: 4.4837 - acc: 0.7188 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 243s 2s/step - loss: 4.5172 - acc: 0.7167 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 244s 2s/step - loss: 4.5082 - acc: 0.7172 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 243s 2s/step - loss: 4.5137 - acc: 0.7169 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 5.314128398895264\n",
      "Test accuracy: 0.6666666865348816\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:8, rotation_range:40, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 260s 2s/step - loss: 4.4810 - acc: 0.7163 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 246s 2s/step - loss: 4.4928 - acc: 0.7182 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 245s 2s/step - loss: 4.5349 - acc: 0.7155 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 240s 2s/step - loss: 4.5238 - acc: 0.7162 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 242s 2s/step - loss: 4.4837 - acc: 0.7188 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 243s 2s/step - loss: 4.5172 - acc: 0.7167 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 244s 2s/step - loss: 4.5082 - acc: 0.7172 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 241s 2s/step - loss: 4.5137 - acc: 0.7169 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 5.314128398895264\n",
      "Test accuracy: 0.6666666865348816\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:8, rotation_range:50, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 264s 2s/step - loss: 4.4810 - acc: 0.7163 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 244s 2s/step - loss: 4.4928 - acc: 0.7182 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 240s 2s/step - loss: 4.5349 - acc: 0.7155 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 243s 2s/step - loss: 4.5238 - acc: 0.7162 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 242s 2s/step - loss: 4.4837 - acc: 0.7188 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 245s 2s/step - loss: 4.5172 - acc: 0.7167 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 244s 2s/step - loss: 4.5082 - acc: 0.7172 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 240s 2s/step - loss: 4.5137 - acc: 0.7169 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 5.314128398895264\n",
      "Test accuracy: 0.6666666865348816\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:8, rotation_range:50, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 259s 2s/step - loss: 4.4810 - acc: 0.7163 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 241s 2s/step - loss: 4.1810 - acc: 0.7104 - val_loss: 0.6247 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 244s 2s/step - loss: 0.5865 - acc: 0.7281 - val_loss: 0.4968 - val_acc: 0.7292\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 244s 2s/step - loss: 0.4633 - acc: 0.7922 - val_loss: 1.3630 - val_acc: 0.5781\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 244s 2s/step - loss: 0.3477 - acc: 0.8611 - val_loss: 1.7351 - val_acc: 0.7109\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 242s 2s/step - loss: 0.2614 - acc: 0.9006 - val_loss: 2.3156 - val_acc: 0.5547\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 241s 2s/step - loss: 0.2431 - acc: 0.9049 - val_loss: 2.6034 - val_acc: 0.5755\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 241s 2s/step - loss: 0.2406 - acc: 0.9256 - val_loss: 1.7188 - val_acc: 0.6901\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 1.568038610741496\n",
      "Test accuracy: 0.6875000074505806\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:16, epochs:8, rotation_range:50, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 259s 2s/step - loss: 4.4810 - acc: 0.7163 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 243s 2s/step - loss: 4.4928 - acc: 0.7182 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 241s 2s/step - loss: 4.5349 - acc: 0.7155 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 242s 2s/step - loss: 4.5238 - acc: 0.7162 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 241s 2s/step - loss: 4.4837 - acc: 0.7188 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 242s 2s/step - loss: 4.5172 - acc: 0.7167 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 240s 2s/step - loss: 4.5082 - acc: 0.7172 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 242s 2s/step - loss: 4.5137 - acc: 0.7169 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "---\n",
      "32/32 [==============================] - 40s 1s/step\n",
      "Test loss: 5.314128398895264\n",
      "Test accuracy: 0.6666666865348816\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:4, rotation_range:40, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 181s 3s/step - loss: 4.6247 - acc: 0.7052 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 169s 3s/step - loss: 4.5783 - acc: 0.7128 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 171s 3s/step - loss: 4.3516 - acc: 0.7270 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 194s 3s/step - loss: 4.5487 - acc: 0.7147 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "---\n",
      "16/16 [==============================] - 22s 1s/step\n",
      "Test loss: 5.314128398895264\n",
      "Test accuracy: 0.6666666865348816\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:4, rotation_range:40, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 189s 3s/step - loss: 2.5674 - acc: 0.6800 - val_loss: 0.5736 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 172s 3s/step - loss: 0.5685 - acc: 0.7345 - val_loss: 0.7516 - val_acc: 0.6458\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 170s 3s/step - loss: 0.4223 - acc: 0.7970 - val_loss: 1.1742 - val_acc: 0.5781\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 169s 3s/step - loss: 0.3581 - acc: 0.8421 - val_loss: 1.1543 - val_acc: 0.5104\n",
      "---\n",
      "16/16 [==============================] - 21s 1s/step\n",
      "Test loss: 1.1723549403250217\n",
      "Test accuracy: 0.5156249962747097\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:4, rotation_range:40, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 186s 3s/step - loss: 0.8464 - acc: 0.6767 - val_loss: 0.5496 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 179s 3s/step - loss: 0.5736 - acc: 0.7357 - val_loss: 0.8015 - val_acc: 0.5885\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 179s 3s/step - loss: 0.4319 - acc: 0.7959 - val_loss: 0.9781 - val_acc: 0.4427\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 177s 3s/step - loss: 0.3653 - acc: 0.8415 - val_loss: 0.8237 - val_acc: 0.5833\n",
      "---\n",
      "16/16 [==============================] - 21s 1s/step\n",
      "Test loss: 0.8240719884634018\n",
      "Test accuracy: 0.5937499962747097\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:4, rotation_range:50, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 188s 3s/step - loss: 0.8233 - acc: 0.6836 - val_loss: 0.8091 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 171s 3s/step - loss: 0.5873 - acc: 0.7283 - val_loss: 0.5674 - val_acc: 0.5573\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 169s 3s/step - loss: 0.5062 - acc: 0.8038 - val_loss: 0.5592 - val_acc: 0.5990\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 169s 3s/step - loss: 0.3799 - acc: 0.8302 - val_loss: 1.7884 - val_acc: 0.5885\n",
      "---\n",
      "16/16 [==============================] - 20s 1s/step\n",
      "Test loss: 1.795314997434616\n",
      "Test accuracy: 0.567708320915699\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:4, rotation_range:50, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 183s 3s/step - loss: 0.8800 - acc: 0.6618 - val_loss: 0.6007 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 177s 3s/step - loss: 0.5940 - acc: 0.7192 - val_loss: 0.5556 - val_acc: 0.6146\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 178s 3s/step - loss: 0.5096 - acc: 0.7647 - val_loss: 0.7163 - val_acc: 0.5156\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 177s 3s/step - loss: 0.4489 - acc: 0.7951 - val_loss: 1.0881 - val_acc: 0.5104\n",
      "---\n",
      "16/16 [==============================] - 21s 1s/step\n",
      "Test loss: 1.0794248655438423\n",
      "Test accuracy: 0.5520833227783442\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:4, rotation_range:50, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/4\n",
      "64/64 [==============================] - 184s 3s/step - loss: 0.8476 - acc: 0.6641 - val_loss: 0.6555 - val_acc: 0.6667\n",
      "Epoch 2/4\n",
      "64/64 [==============================] - 174s 3s/step - loss: 0.6128 - acc: 0.7377 - val_loss: 0.6589 - val_acc: 0.6615\n",
      "Epoch 3/4\n",
      "64/64 [==============================] - 183s 3s/step - loss: 0.4523 - acc: 0.7908 - val_loss: 1.1536 - val_acc: 0.4583\n",
      "Epoch 4/4\n",
      "64/64 [==============================] - 179s 3s/step - loss: 0.3980 - acc: 0.8102 - val_loss: 1.1400 - val_acc: 0.5156\n",
      "---\n",
      "16/16 [==============================] - 22s 1s/step\n",
      "Test loss: 1.1084177531301975\n",
      "Test accuracy: 0.5052083283662796\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:8, rotation_range:40, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 186s 3s/step - loss: 4.6177 - acc: 0.7017 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 177s 3s/step - loss: 4.5783 - acc: 0.7128 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 178s 3s/step - loss: 4.3516 - acc: 0.7270 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 186s 3s/step - loss: 4.5487 - acc: 0.7147 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 178s 3s/step - loss: 4.4699 - acc: 0.7196 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 187s 3s/step - loss: 4.4995 - acc: 0.7178 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 207s 3s/step - loss: 4.4699 - acc: 0.7196 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 196s 3s/step - loss: 4.5093 - acc: 0.7171 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "---\n",
      "16/16 [==============================] - 22s 1s/step\n",
      "Test loss: 5.314128398895264\n",
      "Test accuracy: 0.6666666865348816\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:8, rotation_range:40, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 199s 3s/step - loss: 4.6256 - acc: 0.6998 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 193s 3s/step - loss: 4.5783 - acc: 0.7128 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 228s 4s/step - loss: 4.3516 - acc: 0.7270 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 196s 3s/step - loss: 4.5487 - acc: 0.7147 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 177s 3s/step - loss: 4.4699 - acc: 0.7196 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 178s 3s/step - loss: 4.4995 - acc: 0.7178 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 179s 3s/step - loss: 4.4699 - acc: 0.7196 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 178s 3s/step - loss: 4.5093 - acc: 0.7171 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "---\n",
      "16/16 [==============================] - 22s 1s/step\n",
      "Test loss: 5.314128398895264\n",
      "Test accuracy: 0.6666666865348816\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:8, rotation_range:40, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 189s 3s/step - loss: 4.6251 - acc: 0.7027 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 177s 3s/step - loss: 4.5783 - acc: 0.7128 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 177s 3s/step - loss: 4.3516 - acc: 0.7270 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 179s 3s/step - loss: 2.7711 - acc: 0.6969 - val_loss: 0.6326 - val_acc: 0.6667\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 178s 3s/step - loss: 0.6131 - acc: 0.7205 - val_loss: 0.5502 - val_acc: 0.6667\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 180s 3s/step - loss: 0.4731 - acc: 0.7645 - val_loss: 0.5754 - val_acc: 0.6510\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 179s 3s/step - loss: 0.4054 - acc: 0.8054 - val_loss: 1.4360 - val_acc: 0.5573\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 179s 3s/step - loss: 0.3449 - acc: 0.8531 - val_loss: 3.3383 - val_acc: 0.6302\n",
      "---\n",
      "16/16 [==============================] - 22s 1s/step\n",
      "Test loss: 3.348940223455429\n",
      "Test accuracy: 0.6354166716337204\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:8, rotation_range:50, zoom_range:0.2\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 189s 3s/step - loss: 4.0271 - acc: 0.6915 - val_loss: 1.1747 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 180s 3s/step - loss: 0.6281 - acc: 0.7138 - val_loss: 0.6047 - val_acc: 0.6302\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 179s 3s/step - loss: 0.5111 - acc: 0.7361 - val_loss: 0.6654 - val_acc: 0.6667\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 180s 3s/step - loss: 0.4013 - acc: 0.7864 - val_loss: 1.0716 - val_acc: 0.4896\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 179s 3s/step - loss: 0.3606 - acc: 0.8284 - val_loss: 1.1635 - val_acc: 0.6771\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 179s 3s/step - loss: 0.2871 - acc: 0.8798 - val_loss: 2.4339 - val_acc: 0.5729\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 180s 3s/step - loss: 0.2076 - acc: 0.9184 - val_loss: 2.5690 - val_acc: 0.6250\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 180s 3s/step - loss: 0.1818 - acc: 0.9421 - val_loss: 2.8400 - val_acc: 0.5781\n",
      "---\n",
      "16/16 [==============================] - 22s 1s/step\n",
      "Test loss: 2.809299886226654\n",
      "Test accuracy: 0.5781249813735485\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:8, rotation_range:50, zoom_range:0.3\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 188s 3s/step - loss: 4.6250 - acc: 0.7037 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 179s 3s/step - loss: 4.5783 - acc: 0.7128 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 189s 3s/step - loss: 4.3516 - acc: 0.7270 - val_loss: 5.3141 - val_acc: 0.6667\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 179s 3s/step - loss: 2.7775 - acc: 0.7023 - val_loss: 0.6379 - val_acc: 0.6667\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 187s 3s/step - loss: 0.6186 - acc: 0.7052 - val_loss: 0.5998 - val_acc: 0.6667\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 189s 3s/step - loss: 0.5576 - acc: 0.7271 - val_loss: 0.7158 - val_acc: 0.6667\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 188s 3s/step - loss: 0.5950 - acc: 0.7270 - val_loss: 2.5095 - val_acc: 0.6667\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 195s 3s/step - loss: 0.4660 - acc: 0.7543 - val_loss: 0.9708 - val_acc: 0.6667\n",
      "---\n",
      "16/16 [==============================] - 22s 1s/step\n",
      "Test loss: 0.9826850518584251\n",
      "Test accuracy: 0.6666666865348816\n",
      "-----\n",
      "Trying model with image_size:512, batch_size:32, epochs:8, rotation_range:50, zoom_range:0.4\n",
      "Found 46 images belonging to 2 classes.\n",
      "Found 12 images belonging to 2 classes.\n",
      "Epoch 1/8\n",
      "64/64 [==============================] - 199s 3s/step - loss: 0.8958 - acc: 0.6680 - val_loss: 0.5711 - val_acc: 0.6667\n",
      "Epoch 2/8\n",
      "64/64 [==============================] - 185s 3s/step - loss: 0.5882 - acc: 0.7238 - val_loss: 0.5517 - val_acc: 0.6719\n",
      "Epoch 3/8\n",
      "64/64 [==============================] - 180s 3s/step - loss: 0.5786 - acc: 0.7386 - val_loss: 0.9060 - val_acc: 0.6771\n",
      "Epoch 4/8\n",
      "64/64 [==============================] - 203s 3s/step - loss: 0.4857 - acc: 0.7252 - val_loss: 0.5936 - val_acc: 0.6667\n",
      "Epoch 5/8\n",
      "64/64 [==============================] - 186s 3s/step - loss: 0.4605 - acc: 0.7517 - val_loss: 0.9824 - val_acc: 0.6615\n",
      "Epoch 6/8\n",
      "64/64 [==============================] - 231s 4s/step - loss: 0.3981 - acc: 0.7783 - val_loss: 0.4835 - val_acc: 0.6094\n",
      "Epoch 7/8\n",
      "64/64 [==============================] - 207s 3s/step - loss: 0.3666 - acc: 0.8155 - val_loss: 0.9817 - val_acc: 0.6250\n",
      "Epoch 8/8\n",
      "64/64 [==============================] - 189s 3s/step - loss: 0.3234 - acc: 0.8576 - val_loss: 2.9328 - val_acc: 0.5885\n",
      "---\n",
      "16/16 [==============================] - 22s 1s/step\n",
      "Test loss: 2.9554922357201576\n",
      "Test accuracy: 0.5833333283662796\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "combinations = [image_sizes, batch_sizes, epochs, rotation_ranges, zoom_ranges]\n",
    "for (image_size, batch_size, epochs, rotation_range, zoom_range) in itertools.product(*combinations):\n",
    "    print(\"Trying model with image_size:\" + str(image_size) + \", batch_size:\" + str(batch_size) + \", epochs:\" + str(epochs) + \", rotation_range:\" + str(rotation_range) + \", zoom_range:\" + str(zoom_range))\n",
    "    \n",
    "    model = get_model_with_input_shape(image_size, image_size)\n",
    "    train_generator, validation_generator = get_generators(image_size, image_size, batch_size, rotation_range, zoom_range)\n",
    "    train_model(model, train_generator, validation_generator, batch_size, epochs)\n",
    "    print(\"---\")\n",
    "    results[(image_size, batch_size, epochs, rotation_range, zoom_range)] = evaluate_model(model, validation_generator, batch_size)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination that produces highest test accuracy (0.7343750037252903): (256, 16, 4, 40, 0.4)\n",
      "Combination that produces lowest test loss (0.5065992418676615): (256, 32, 4, 50, 0.3)\n"
     ]
    }
   ],
   "source": [
    "results_acc = {key: value[1] for key, value in results.items()}\n",
    "highest_acc_comb = max(results_acc, key=results_acc.get)\n",
    "print(\"Combination that produces highest test accuracy (\"+str(results_acc[highest_acc_comb])+\"): \" + str(highest_acc_comb))\n",
    "\n",
    "results_loss = {key: value[0] for key, value in results.items()}\n",
    "lowest_loss_comb = min(results_loss, key=results_loss.get)\n",
    "print(\"Combination that produces lowest test loss (\"+str(results_loss[lowest_loss_comb])+\"): \" + str(lowest_loss_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"results.pkl\", \"wb\") as results_file:\n",
    "    pickle.dump(results, results_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_148 (Conv2D)          (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 254, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_148 (MaxPoolin (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_149 (Conv2D)          (None, 125, 125, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_149 (MaxPoolin (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_150 (Conv2D)          (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 60, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_150 (MaxPoolin (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 64)                3686464   \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,715,169\n",
      "Trainable params: 3,715,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_model_with_input_shape(highest_acc_comb[0], highest_acc_comb[0]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
